---
show: step
version: 1.0
enable_checker: true
---

# xpath 路径表达式
## 回忆

- 这次真的爬了一个网站
	- oeasy.org
- 右键检查元素
	- 获取xpath
- 爬取之后获得属性href的值
- 然后切片并拼接为绝对链接地址
- 能把这些链接都爬一遍么？🤔

### 遍历
```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.read().split("\n")
for url in urls:
    response = requests.get(url)
    s_html = response.content.decode("utf-8")
    et_html = etree.HTML(s_html)
    et_target = et_html.xpath(".")
    for element in et_target:
        print(element.tag)
        #attributes = element.attrib
        #f.write("http://localhost/oeasyorg"+attributes["href"][1:] + "\n")
```
- 运行出现了12次html节点
- 但是后面报了错误
- 最后一个url出了问题

### 解决

- 问题可能是由于最后一个链接后面有个`\n`
- 造成了最后的元素是一个空字符串
- 使用切片的方法解决问题

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210902-1630590728046)

### 找到xpath

- 右键工具软件
- 复制xpath
- 修改代码
- 按照地址
	- 输出项目

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210902-1630590988087)

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630651702331)

- 我想做的是保存
	- 课程名
	- 看教程网址

### 遍历
- 打开链接后再存储
	- 各教程名称
	- 看教程网址
```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.read().split("\n")[0:-1]
for url in urls:
    response = requests.get(url)
    s_html = response.content.decode("utf-8")
    et_html = etree.HTML(s_html)
    et_target = et_html.xpath("/html/body/div/div/div[1]")
    et_target2 = et_html.xpath("/html/body/div/div/div[2]/a[1]")
    print("url----"+url)
    count = 0
    while ( count < len(et_target)):
        print(et_target[count].text)
        print(et_target2[count].text)
        print(et_target2[count].attrib["href"])
        count = count + 1
        print("count====" + str(count))
```

- 输出顺利
- 可以把各个网址输出到屏幕
- 现在需要把输出重定向到文本文件

### 输出重定向

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.read().split("\n")[0:-1]
for url in urls:
    response = requests.get(url)
    s_html = response.content.decode("utf-8")
    et_html = etree.HTML(s_html)
    et_target = et_html.xpath("/html/body/div/div/div[1]")
    et_target2 = et_html.xpath("/html/body/div/div/div[2]/a[1]")
    print("url----"+url)
    count = 0
    with open("urls.csv","at") as f:
        while ( count < len(et_target)):
            print("count====" + str(count))
            print(et_target[count].text)
            print(et_target2[count].text)
            print(et_target2[count].attrib["href"])
            s_url = et_target[count].text + "," \
                  + et_target2[count].attrib["href"] + "\n"
            f.write(s_url)
            count = count + 1
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630653569803)

### 优化代码

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.read().split("\n")[0:-1]
for url in urls:
    response = requests.get(url)
    s_html = response.content.decode("utf-8")
    et_html = etree.HTML(s_html)
    et_target = et_html.xpath("/html/body/div/div/div[1]")
    et_target2 = et_html.xpath("/html/body/div/div/div[2]/a[1]")
    count = len(et_target)
    with open("urls.csv","at") as f:
        for cur in range(0,count-1):
            print("cur====" + str(cur))
            print(et_target[cur].text)
            print(et_target2[cur].text)
            print(et_target2[cur].attrib["href"])
            s_url = et_target[cur].text + "," \
                  + et_target2[cur].attrib["href"] + "\n"
            f.write(s_url)
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630663926329)

### 入库
- 更新系统安装postgres
- 启动postgres
- 并以postgres登录

```
sudo apt update
sudo apt install postgresql
sudo /etc/init.d/postgres start
sudo -u postgres psql
```

- 查看所有数据库database

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630664944685)

### 查看数据库
```
\l
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665003642)

- 建立数据库oeasy
- 并进入

```
create database oeasy;
\l
```
![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665050881)

### 建表并导入数据
```
\c oeasy
\dt
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665134696)

- 建表

```
create table urls(topic varchar, url varchar);
\dt
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665232801)

- 导入数据

```
\copy urls from '/home/shiyanlou/urls.csv' delimiter ',' csv ;
```
![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665741698)

- 查询数据

```
select * from urls where topic like '%e%';
select * from urls where length(topic) > '5';
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665711812)

### 总结

- 这次真的爬了一个网站
	- oeasy.org
- 右键检查元素
	- 获取xpath
- 爬取之后获得属性href的值
- 然后切片并拼接为绝对链接地址
- 并且把每一个链接都爬了一遍
- 能出去爬个百度么？🤔
- 下次再说
