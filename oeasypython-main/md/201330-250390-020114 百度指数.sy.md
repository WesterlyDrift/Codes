---
show: step
version: 1.0
enable_checker: true
---

# 爬取百度指数
## 回忆
- 这次爬了baidu.com
- 找到了三组链接
- 然后分别遍历	
- 还可以爬点什么？🤔

### 搜索

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211024-1635081650764)

- 百度指数的加密方法经常修改
- 所以要搜索最新版本的爬虫程序

### 找到这样两篇

https://blog.csdn.net/weixin_43933556/article/details/118163875
https://blog.csdn.net/Buhdda/article/details/83864596

- 我们可以自己试试

### 尝试的结果

```python
import datetime
import requests


# 搜索指数数据解密
def decryption(keys, data):
    dec_dict = {}
    for j in range(len(keys) // 2):
        dec_dict[keys[j]] = keys[len(keys) // 2 + j]

    dec_data = ''
    for k in range(len(data)):
        dec_data += dec_dict[data[k]]
    return dec_data


if __name__ == "__main__":
    scenicName = 'oeasy'

    dataUrl = 'https://index.baidu.com/api/SearchApi/index?area=0&word=[[%7B%22name%22:%22' + scenicName + '%22,%22wordType%22:1%7D]]&days=365'

    keyUrl = 'https://index.baidu.com/Interface/ptbk?uniqid='
    header = {
            "Host": "index.baidu.com",
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:93.0) Gecko/20100101 Firefox/93.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Referer": "http://index.baidu.com/",
            "Cookie": "BIDUPSID=5A",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "cross-site"
            }
    # 设置请求超时时间为30秒
    resData = requests.get(dataUrl, timeout=30, headers=header)
    print(resData.content)
    uniqid = resData.json()['data']['uniqid']
    print("uniqid:{}".format(uniqid))
    keyData = requests.get(keyUrl + uniqid, timeout=30, headers=header)
    print("keyData",keyData.content)
    keyData.raise_for_status()
    keyData.encoding = resData.apparent_encoding

    # 开始对json数据进行解析
    startDate = resData.json()['data']['userIndexes'][0]['all']['startDate']
    print("startDate:{}".format(startDate))
    endDate = resData.json()['data']['userIndexes'][0]['all']['endDate']
    print("endDate:{}".format(endDate))
    source = (resData.json()['data']['userIndexes'][0]['all']['data'])  # 原加密数据
    print("原加密数据:{}".format(source))
    key = keyData.json()['data']  # 密钥
    print("密钥:{}".format(key))


    res = decryption(key, source)
    print("res",type(res),res)
    resArr = res.split(",")
    print(resArr)
    dateStart = datetime.datetime.strptime(startDate, '%Y-%m-%d')
    dateEnd = datetime.datetime.strptime(endDate, '%Y-%m-%d')
    dataLs = []
    print(dateStart,dateEnd)
    while dateStart <= dateEnd:
        dataLs.append(str(dateStart))
        dateStart += datetime.timedelta(days=1)
        #print(dateStart.strftime('%Y-%m-%d'))

    ls = []
    for i in range(len(dataLs)):
        ls.append([scenicName, dataLs[i], resArr[i]])

    for i in range(len(ls)):
        print(ls[i])
```

### 尝试总结

- 需要把Cookie中的"换成\"
- 目前只能得到一年以内的数据
- 再次搜索
- https://blog.csdn.net/qq_30440287/article/details/120373441
### 修改代码

```
import random
import time

import requests
import execjs
from datetime import datetime, timedelta
import pandas as pd

class BaiDuZhiShu():
    def __init__(self, table):
        self.headers = {
                "Host": "index.baidu.com",
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:93.0) Gecko/20100101 Firefox/93.0",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Referer": "http://index.baidu.com/",
                "Cookie": "BIDUPSID=5A",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "cross-site"
                }

        self.table = table
        self.i = 1
        self.df = None

    # 从当前日期到结束日期的天列表
    def get_time_days(self, start, end):
        days_list = []
        start = datetime.strptime(start, '%Y-%m-%d')
        end = datetime.strptime(end, '%Y-%m-%d')
        while start <= end:
            days_list.append(start.strftime('%Y-%m-%d'))
            start += timedelta(days=1)
        return days_list

    def decrypt(self, t, e):
        js_decrypt = '''
                function decrypt(t, e) {
            if (t) {
                for (var n = t.split(""), i = e.split(""), a = {}, r = [], o = 0; o < n.length / 2; o++)
                    a[n[o]] = n[n.length / 2 + o];
                for (var s = 0; s < e.length; s++)
                    r.push(a[i[s]]);
                return r.join("")
            }
        }
        '''
        compile = execjs.compile(js_decrypt)
        data = compile.call('decrypt', t, e)
        list_1 = []
        for i in data.split(','):
            if i:
                list_1.append(int(i))
            else:
                list_1.append(int(0))

        return list_1

    def get_t(self, uniqid):
        url = f'https://index.baidu.com/Interface/ptbk?uniqid={uniqid}'
        return requests.get(url, headers=self.headers).json()['data']

    def get_csv(self, name, decrypt_data, days_list):
        # name 为列名
        if len(decrypt_data) == len(days_list):
            list_data = [{"time": days_list[i], name: decrypt_data[i]} for i in range(len(days_list))]
            list_1 = []
            list_2 = []
            for i in list_data:
                list_1.append(i['time'])
                list_2.append(i[name])
            # self.get_shuju(name, pd.DataFrame({'time': list_1, name: list_2}))
            # self.df.to_csv('./百度热搜电影榜.csv')
            # 更新数据
            #self.update(name, days_list, list_2)

            return pd.DataFrame({'time': list_1, name: list_2})

    def run(self, name, start_day, end_day):
        '''
            :start_day:开始时间，例如2020-09-03
            :end_day:结束时间，例如2021-09-03
        '''
        days_list = self.get_time_days(start_day, end_day)
        # {}花括号转移为两个花括号
        url = f'https://index.baidu.com/api/SearchApi/index?area=0&word=[[{{"name":"{name}","wordType":1}}]]&startDate={start_day}&endDate={end_day}'
        page = requests.get(url, headers=self.headers).json()
        uniqid = page['data']['uniqid']
        t = self.get_t(uniqid)
        e = page['data']['userIndexes'][0]['all']['data']
        decrypt_data = self.decrypt(t, e)

        dataframe = self.get_csv(name, decrypt_data, days_list)
        self.get_shuju(name, dataframe)

    def get_shuju(self, name, dataframe):
        if self.i == 1:
            self.df = dataframe
            self.i += 1
        else:
            self.df[name] = dataframe[name]

    # 判断是否有表
if __name__ == '__main__':
    bd = BaiDuZhiShu('明星')
    for name in ['王俊凯','王源','易烊千玺']:
        bd.run(name, '2011-01-01', '2011-12-31')
    # 保存为csv格式
    bd.df.to_csv(f'./{bd.table}.csv')
    print(bd.df)

```

### 处理

- Cookie需要进行处理"变成\"
- 把入库的内容排除
- 一年一年的爬取
- 然后再合并
- 后期可以和flourish合并到一起
- 做成racingbar那样的图
- 那百度指数是否可以和地区相关联？

### 地区指数

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211024-1635083970422)

- 我们可以得到一个字典
- 里面有省、地级市和具体城市的信息
- 但不知道城市代码和具体城市名称的关联
- 搜索广东 913

### 找到关联
```python
 
import json
 
# 0. 全局默认变量
MAX_KEYWORD_GROUP_NUMBER = 5											 # 最多可以将MAX_KEYWORD_GROUP_NUMBER组关键词进行比较
MAX_KEYWORD_GROUP_SIZE = 3												 # 最多可以将MAX_KEYWORD_GROUP_SIZE个关键词进行组合
MAX_DATE_LENGTH = 360													 # 日期区间长度应当小于366, 取个整数就用360天即可
########################################################################
 
# 1. 百度指数js常用变量
## 1.1 省份区域索引表: CODE2AREA
##   - CODE2PROVINCE来源: main-vendor.xxxxxxxxxxxxxxxxxxx.js源码中直接复制可得
##   - js源码URL: e.g. http://index.baidu.com/v2/static/js/main-vendor.dbf1ed721cfef2e2755d.js
CODE2AREA = {
	901:"华东",902:"西南",903:"华中",904:"西南",905:"华北",
	906:"华中",907:"东北",908:"华中",909:"华东",910:"华东",
	911:"华北",912:"华南",913:"华南",914:"西南",915:"西南",
	916:"华东",917:"华东",918:"西北",919:"西北",920:"华北",
	921:"东北",922:"东北",923:"华北",924:"西北",925:"西北",
	926:"西北",927:"华中",928:"华东",929:"华北",930:"华南",
	931:"华南",932:"西南",933:"华南",934:"华南",
}
 
## 1.2 省份编号索引表: CODE2PROVINCE, PROVINCE2CODE
##   - CODE2PROVINCE来源: main-vendor.xxxxxxxxxxxxxxxxxxx.js源码中直接复制可得
##   - js源码URL: e.g. http://index.baidu.com/v2/static/js/main-vendor.dbf1ed721cfef2e2755d.js
CODE2PROVINCE = {														
	901:"山东",902:"贵州",903:"江西",904:"重庆",905:"内蒙古",
	906:"湖北",907:"辽宁",908:"湖南",909:"福建",910:"上海",
	911:"北京",912:"广西",913:"广东",914:"四川",915:"云南",
	916:"江苏",917:"浙江",918:"青海",919:"宁夏",920:"河北",
	921:"黑龙江",922:"吉林",923:"天津",924:"陕西",925:"甘肃",
	926:"新疆",927:"河南",928:"安徽",929:"山西",930:"海南",
	931:"台湾",932:"西藏",933:"香港",934:"澳门",
}
 
PROVINCE2CODE = {province:str(code) for code,province in CODE2PROVINCE.items()}
## 1.3 城市编号索引表: CODE2CITY, CITY2CODE
##   - CODE2PROVINCE来源: main-vendor.xxxxxxxxxxxxxxxxxxx.js源码中直接复制可得
##   - js源码URL: e.g. http://index.baidu.com/v2/static/js/main-vendor.dbf1ed721cfef2e2755d.js
CODE2CITY = {
	1:"济南",2:"贵阳",3:"黔南",4:"六盘水",5:"南昌",
	6:"九江",7:"鹰潭",8:"抚州",9:"上饶",10:"赣州",
	11:"重庆",13:"包头",14:"鄂尔多斯",15:"巴彦淖尔",16:"乌海",
	17:"阿拉善盟",19:"锡林郭勒盟",20:"呼和浩特",21:"赤峰",22:"通辽",
	25:"呼伦贝尔",28:"武汉",29:"大连",30:"黄石",31:"荆州",
	32:"襄阳",33:"黄冈",34:"荆门",35:"宜昌",36:"十堰",
	37:"随州",38:"恩施",39:"鄂州",40:"咸宁",41:"孝感",
	42:"仙桃",43:"长沙",44:"岳阳",45:"衡阳",46:"株洲",
	47:"湘潭",48:"益阳",49:"郴州",50:"福州",51:"莆田",
	52:"三明",53:"龙岩",54:"厦门",55:"泉州",56:"漳州",
	57:"上海",59:"遵义",61:"黔东南",65:"湘西",66:"娄底",
	67:"怀化",68:"常德",73:"天门",74:"潜江",76:"滨州",
	77:"青岛",78:"烟台",79:"临沂",80:"潍坊",81:"淄博",
	82:"东营",83:"聊城",84:"菏泽",85:"枣庄",86:"德州",
	87:"宁德",88:"威海",89:"柳州",90:"南宁",91:"桂林",
	92:"贺州",93:"贵港",94:"深圳",95:"广州",96:"宜宾",
	97:"成都",98:"绵阳",99:"广元",100:"遂宁",101:"巴中",
	102:"内江",103:"泸州",104:"南充",106:"德阳",107:"乐山",
	108:"广安",109:"资阳",111:"自贡",112:"攀枝花",113:"达州",
	114:"雅安",115:"吉安",117:"昆明",118:"玉林",119:"河池",
	123:"玉溪",124:"楚雄",125:"南京",126:"苏州",127:"无锡",
	128:"北海",129:"钦州",130:"防城港",131:"百色",132:"梧州",
	133:"东莞",134:"丽水",135:"金华",136:"萍乡",137:"景德镇",
	138:"杭州",139:"西宁",140:"银川",141:"石家庄",143:"衡水",
	144:"张家口",145:"承德",146:"秦皇岛",147:"廊坊",148:"沧州",
	149:"温州",150:"沈阳",151:"盘锦",152:"哈尔滨",153:"大庆",
	154:"长春",155:"四平",156:"连云港",157:"淮安",158:"扬州",
	159:"泰州",160:"盐城",161:"徐州",162:"常州",163:"南通",
	164:"天津",165:"西安",166:"兰州",168:"郑州",169:"镇江",
	172:"宿迁",173:"铜陵",174:"黄山",175:"池州",176:"宣城",
	177:"巢湖",178:"淮南",179:"宿州",181:"六安",182:"滁州",
	183:"淮北",184:"阜阳",185:"马鞍山",186:"安庆",187:"蚌埠",
	188:"芜湖",189:"合肥",191:"辽源",194:"松原",195:"云浮",
	196:"佛山",197:"湛江",198:"江门",199:"惠州",200:"珠海",
	201:"韶关",202:"阳江",203:"茂名",204:"潮州",205:"揭阳",
	207:"中山",208:"清远",209:"肇庆",210:"河源",211:"梅州",
	212:"汕头",213:"汕尾",215:"鞍山",216:"朝阳",217:"锦州",
	218:"铁岭",219:"丹东",220:"本溪",221:"营口",222:"抚顺",
	223:"阜新",224:"辽阳",225:"葫芦岛",226:"张家界",227:"大同",
	228:"长治",229:"忻州",230:"晋中",231:"太原",232:"临汾",
	233:"运城",234:"晋城",235:"朔州",236:"阳泉",237:"吕梁",
	239:"海口",241:"万宁",242:"琼海",243:"三亚",244:"儋州",
	246:"新余",253:"南平",256:"宜春",259:"保定",261:"唐山",
	262:"南阳",263:"新乡",264:"开封",265:"焦作",266:"平顶山",
	268:"许昌",269:"永州",270:"吉林",271:"铜川",272:"安康",
	273:"宝鸡",274:"商洛",275:"渭南",276:"汉中",277:"咸阳",
	278:"榆林",280:"石河子",281:"庆阳",282:"定西",283:"武威",
	284:"酒泉",285:"张掖",286:"嘉峪关",287:"台州",288:"衢州",
	289:"宁波",291:"眉山",292:"邯郸",293:"邢台",295:"伊春",
	297:"大兴安岭",300:"黑河",301:"鹤岗",302:"七台河",303:"绍兴",
	304:"嘉兴",305:"湖州",306:"舟山",307:"平凉",308:"天水",
	309:"白银",310:"吐鲁番",311:"昌吉",312:"哈密",315:"阿克苏",
	317:"克拉玛依",318:"博尔塔拉",319:"齐齐哈尔",320:"佳木斯",322:"牡丹江",
	323:"鸡西",324:"绥化",331:"乌兰察布",333:"兴安盟",334:"大理",
	335:"昭通",337:"红河",339:"曲靖",342:"丽江",343:"金昌",
	344:"陇南",346:"临夏",350:"临沧",352:"济宁",353:"泰安",
	356:"莱芜",359:"双鸭山",366:"日照",370:"安阳",371:"驻马店",
	373:"信阳",374:"鹤壁",375:"周口",376:"商丘",378:"洛阳",
	379:"漯河",380:"濮阳",381:"三门峡",383:"阿勒泰",384:"喀什",
	386:"和田",391:"亳州",395:"吴忠",396:"固原",401:"延安",
	405:"邵阳",407:"通化",408:"白山",410:"白城",417:"甘孜",
	422:"铜仁",424:"安顺",426:"毕节",437:"文山",438:"保山",
	456:"东方",457:"阿坝",466:"拉萨",467:"乌鲁木齐",472:"石嘴山",
	479:"凉山",480:"中卫",499:"巴音郭楞",506:"来宾",514:"北京",
	516:"日喀则",520:"伊犁",525:"延边",563:"塔城",582:"五指山",
	588:"黔西南",608:"海西",652:"海东",653:"克孜勒苏柯尔克孜",654:"天门仙桃",
	655:"那曲",656:"林芝",657:"None",658:"防城",659:"玉树",
	660:"伊犁哈萨克",661:"五家渠",662:"思茅",663:"香港",664:"澳门",
	665:"崇左",666:"普洱",667:"济源",668:"西双版纳",669:"德宏",
	670:"文昌",671:"怒江",672:"迪庆",673:"甘南",674:"陵水黎族自治县",
	675:"澄迈县",676:"海南",677:"山南",678:"昌都",679:"乐东黎族自治县",
	680:"临高县",681:"定安县",682:"海北",683:"昌江黎族自治县",684:"屯昌县",
	685:"黄南",686:"保亭黎族苗族自治县",687:"神农架",688:"果洛",689:"白沙黎族自治县",
	690:"琼中黎族苗族自治县",691:"阿里",692:"阿拉尔",693:"图木舒克",
}
```

### 原始数据
- 找到provinceReal可以找到原始数据
- 然后查字典

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20211024-1635084875883)

```
		"901": 423,
		"902": 109,
		"903": 233,
		"904": 197,
		"905": 75,
		"906": 304,
		"907": 156,
		"908": 284,
		"909": 230,
		"910": 213,
		"911": 283,
		"912": 182,
		"913": 1000,
		"914": 398,
		"915": 130,
		"916": 433,
		"917": 418,
		"918": 21,
		"919": 28,
		"920": 258,
		"921": 103,
		"922": 68,
		"923": 102,
		"924": 210,
		"925": 68,
		"926": 88,
		"927": 450,
		"928": 200,
		"929": 156,
		"930": 42,
		"931": 2,
		"932": 8,
		"933": 4
```

### 总结

- 这次爬了百度指数
- 可以通过时间维度进行分割
- 也可以通过空间维度进行分割
- 重要的是能够到网上查找源代码
- 这很酷啊
- 还能搜点什么呢？🤔
- 下次再说
